{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading\n",
    "\n",
    "We often want to save the parameters of a trained model, so that we can use it again later without needing to re-train. \n",
    "\n",
    "All Feedbax components—including, automatically and for free, any that you might write—are [PyTrees](feedbax/examples/pytrees): they are represented as tree-structured data. Equinox is able to [save](https://docs.kidger.site/equinox/examples/serialisation/) this data to a file. \n",
    "\n",
    "Feedbax provides some functions to make this slightly easier. However, you can also [learn](https://docs.kidger.site/equinox/api/serialisation/) to use the Equinox functions `tree_serialise_leaves` and `tree_deserialize_leaves`, if you prefer a different scheme for saving and loading.\n",
    "\n",
    "Here's an example of how to use the functions provided by Feedbax. \n",
    "\n",
    "We'll start by writing a function that sets up the components we're going to want to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "from feedbax.task import SimpleReaches\n",
    "\n",
    "from feedbax.xabdeef.losses import simple_reach_loss\n",
    "from feedbax.xabdeef.models import point_mass_nn\n",
    "\n",
    "# The leading asterisk forces all the arguments to be passed as keyword arguments\n",
    "def setup(*, workspace, n_steps, dt, hidden_size, key):\n",
    "\n",
    "    task = SimpleReaches(\n",
    "        loss_func=simple_reach_loss(), \n",
    "        workspace=workspace, \n",
    "        n_steps=n_steps\n",
    "    )\n",
    "\n",
    "    model = point_mass_nn(task, dt=dt, hidden_size=hidden_size, key=key)\n",
    "    \n",
    "    return task, model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to get a task and model. We'll also want to save the parameters we use to set up the task and model, so let's store them together in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(\n",
    "    workspace=((-1., -1.),  # Workspace bounds ((x_min, y_min), (x_max, y_max)\n",
    "               (1., 1.)),\n",
    "    n_steps=100,  # Number of time steps per trial\n",
    "    dt=0.05,  # Duration of a time step\n",
    "    hidden_size=50,  # Number of units in the hidden layer of the controller\n",
    ")\n",
    "\n",
    "key_init, key_train = jax.random.split(jax.random.PRNGKey(0))\n",
    "\n",
    "task, model = setup(**hyperparameters, key=key_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model to perform the task. We just do a short training run, since in this case we're interested in saving the model rather than whether it has converged on a solution or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24b7d68adaf4d40b4f5865dd1496121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compile:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step compiled.\n",
      "Validation step compiled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816465f2035b453d95e4a8d69bd4440f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train batch:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training iteration: 0\n",
      "\ttraining loss: 3.94e+01\n",
      "\tvalidation loss: 5.14e+00\n",
      "\n",
      "Training iteration: 100\n",
      "\ttraining loss: 2.86e-02\n",
      "\tvalidation loss: 5.32e-03\n",
      "\n",
      "Training iteration: 200\n",
      "\ttraining loss: 8.28e-03\n",
      "\tvalidation loss: 1.58e-03\n",
      "\n",
      "Training iteration: 300\n",
      "\ttraining loss: 6.48e-03\n",
      "\tvalidation loss: 1.25e-03\n",
      "\n",
      "Training iteration: 400\n",
      "\ttraining loss: 5.37e-03\n",
      "\tvalidation loss: 1.06e-03\n",
      "\n",
      "Training iteration: 500\n",
      "\ttraining loss: 4.46e-03\n",
      "\tvalidation loss: 9.42e-04\n",
      "\n",
      "Training iteration: 600\n",
      "\ttraining loss: 3.98e-03\n",
      "\tvalidation loss: 8.68e-04\n",
      "\n",
      "Training iteration: 700\n",
      "\ttraining loss: 3.58e-03\n",
      "\tvalidation loss: 8.12e-04\n",
      "\n",
      "Training iteration: 800\n",
      "\ttraining loss: 3.34e-03\n",
      "\tvalidation loss: 7.66e-04\n",
      "\n",
      "Training iteration: 900\n",
      "\ttraining loss: 2.94e-03\n",
      "\tvalidation loss: 7.34e-04\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "\n",
    "from feedbax.trainer import TaskTrainer\n",
    "\n",
    "\n",
    "trainer = TaskTrainer(\n",
    "    optimizer=optax.adam(learning_rate=1e-2)\n",
    ")\n",
    "\n",
    "model, _ = trainer(\n",
    "    task=task, \n",
    "    model=model, \n",
    "    n_batches=1000, \n",
    "    batch_size=250, \n",
    "    where_train=lambda model: model.step.net,\n",
    "    key=key_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedbax import save, load\n",
    "\n",
    "model_path = save(\n",
    "    (task, model),\n",
    "    hyperparameters=hyperparameters, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Note    \n",
    "   Python includes the module [`pickle`](https://docs.python.org/3/library/pickle.html), which can save and load entire Python objects without needing to specify, at the time of loading, how those objects were created. This seems convenient, but it is not good practice in general:\n",
    "   \n",
    "   - Upon loading, Python will automatically execute code found in a pickle file, in order to reconstruct the pickled objects. This is a security issue. If someone shares a pickled model with you, they (or an interloper) could insert harmful code into the pickle file, and you may not know it's there until you run it.\n",
    "   - You probably still have to keep track of how the objects in the pickle were created, for your research to be reproducible in detail. It is better to do this explicitly.\n",
    "   - Some of the components we use, such as `lambda` expressions, are not compatible with `pickle`.\n",
    "   \n",
    "   See the Equinox [documentation](https://docs.kidger.site/equinox/examples/serialisation/) for a similar discussion of these limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, task = load(model_path, setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Equinox.\n",
    "\n",
    "Need a function that can construct your model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
