{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From automatic to manual\n",
    "\n",
    "In the first example we successfully trained a model. However, almost everything was pre-built and we didn't get to see how the model was put together. Let's rebuild the same model, but more explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll define the model parameters we might want to vary in this example. \n",
    "\n",
    "Of course, we'll also import JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "\n",
    "\n",
    "mass = 1.0  # Mass of the point mass\n",
    "n_steps = 100  # Number of time steps per trial\n",
    "dt = 0.05  # Duration of a time step\n",
    "feedback_delay_steps = 0  # Number of steps to delay the feedback\n",
    "feedback_noise_std = 0.1  # Standard deviation of Gaussian noise added to sensory feedback\n",
    "workspace = ((-1., -1.),  # Workspace bounds ((x_min, y_min), (x_max, y_max)\n",
    "             (1., 1.))\n",
    "hidden_size  = 50  # Number of units in the hidden layer of the controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define our random keys. We'll use `jax.random.split` to get several keys we can use for different purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 18:50:16.778053: W external/xla/xla/service/platform_util.cc:198] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN: unknown error\n",
      "CUDA backend failed to initialize: INTERNAL: no supported devices found for platform CUDA (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "seed = 0 \n",
    "key = jax.random.PRNGKey(seed)  # This is the parent key\n",
    "\n",
    "# Split into three different keys for initialisation, training, and evaluation\n",
    "key_init, key_train, key_eval = jax.random.split(key, 3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual pairing of a pre-built model with a task\n",
    "\n",
    "Last time we saw that `feedbax.xabdeef` provides some pre-built pairings of tasks and models, which a single line of code can instantiate all at once, along with a `TaskTrainer`.\n",
    "\n",
    "However, we can also manually choose a task to pair with a pre-built model, instead of pairing them automatically in the background. In this case, we'll use `point_mass_nn` to get the model, but define `SimpleReaches` explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mll/.miniforge3/envs/fx/lib/python3.11/site-packages/diffrax/adjoint.py:665: UserWarning: As of Equinox 0.10.7, `equinox.filter_custom_vjp.defvjp` is deprecated in favour of `.def_fwd` and `.def_bwd`. This new API supports symbolic zeros, which allow for more efficient autodifferentiation rules. In particular:\n",
      "- the fwd and bwd functions take an extra `perturbed` argument, which     indicates which primals actually need a gradient. You can use this     to skip computing the gradient for any unperturbed value. (You can     also safely just ignore this if you wish.)\n",
      "- `None` was previously passed to indicate a symbolic zero gradient for     all objects that weren't inexact arrays, but all inexact arrays     always had an array-valued gradient. Now, `None` may also be passed     to indicate that an inexact array has a symbolic zero gradient.\n",
      "  _loop_backsolve.defvjp(_loop_backsolve_fwd, _loop_backsolve_bwd)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'workspace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeedbax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxabdeef\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple_reach_loss\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeedbax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxabdeef\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m point_mass_nn\n\u001b[1;32m      7\u001b[0m task \u001b[38;5;241m=\u001b[39m SimpleReaches(\n\u001b[1;32m      8\u001b[0m     loss_func\u001b[38;5;241m=\u001b[39msimple_reach_loss(),\n\u001b[0;32m----> 9\u001b[0m     workspace\u001b[38;5;241m=\u001b[39m\u001b[43mworkspace\u001b[49m, \n\u001b[1;32m     10\u001b[0m     n_steps\u001b[38;5;241m=\u001b[39mn_steps,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m point_mass_nn(\n\u001b[1;32m     14\u001b[0m     task,\n\u001b[1;32m     15\u001b[0m     dt\u001b[38;5;241m=\u001b[39mdt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     key\u001b[38;5;241m=\u001b[39mkey_init,\n\u001b[1;32m     22\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'workspace' is not defined"
     ]
    }
   ],
   "source": [
    "from feedbax.task import SimpleReaches\n",
    "\n",
    "from feedbax.xabdeef.losses import simple_reach_loss\n",
    "from feedbax.xabdeef.models import point_mass_nn\n",
    "\n",
    "\n",
    "task = SimpleReaches(\n",
    "    loss_func=simple_reach_loss(),\n",
    "    workspace=workspace, \n",
    "    n_steps=n_steps,\n",
    ")\n",
    "\n",
    "model = point_mass_nn(\n",
    "    task,\n",
    "    dt=dt,\n",
    "    mass=mass,\n",
    "    hidden_size=hidden_size, \n",
    "    n_steps=n_steps,\n",
    "    feedback_delay_steps=feedback_delay_steps,\n",
    "    feedback_noise_std=feedback_noise_std,\n",
    "    key=key_init,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that:\n",
    "\n",
    "- the task is given a *loss function*, `loss_func`, which determines how a model's performance on the task is scored. \n",
    "- `task` is passed to `point_mass_nn`, since the input shape of the neural network depends on the input data provided by the task for each trial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use a `TaskTrainer` to train this `model` to perform this `task`, but let's go one level deeper first and define the model without using any functions from `feedbax.xabdeef`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model ourselves using core Feedbax\n",
    "\n",
    "Now we'll rebuild the exact same model, without using the pre-build function `point_mass_nn` that we imported from `feedbax.xabdeef`. We'll write our own function that lets us build copies of a model with a certain structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedbax.bodies import SimpleFeedback\n",
    "from feedbax.iterate import SimpleIterator\n",
    "from feedbax.mechanics.mechanics import Mechanics\n",
    "from feedbax.mechanics.plant import SimplePlant\n",
    "from feedbax.mechanics.skeleton.pointmass import PointMass\n",
    "from feedbax.networks import SimpleNetwork\n",
    "\n",
    "\n",
    "def get_model(\n",
    "    task,\n",
    "    dt: float = 0.05, \n",
    "    mass: float = 1., \n",
    "    hidden_size: int = 50, \n",
    "    n_steps: int = 100, \n",
    "    feedback_delay: int = 0,\n",
    "    feedback_noise_std: float = 0.0,\n",
    "    out_nonlinearity=lambda x: x,\n",
    "    *,  # This asterisk forces us to pass `key` as a keyword argument, without needing to set a default value\n",
    "    key,\n",
    "):   \n",
    "    key1, key2 = jax.random.split(key)\n",
    "\n",
    "    plant = SimplePlant(PointMass(mass=mass))\n",
    "    mechanics = Mechanics(plant, dt)\n",
    "    \n",
    "    feedback_spec = dict(\n",
    "        where=lambda state: (\n",
    "            state.effector.pos,\n",
    "            state.effector.vel,\n",
    "        ),\n",
    "        delay=feedback_delay,\n",
    "        noise_std=feedback_noise_std,\n",
    "    )\n",
    "    \n",
    "    # Determine the network input size automatically\n",
    "    input_size = SimpleFeedback.get_nn_input_size(\n",
    "        task, mechanics, feedback_spec\n",
    "    )\n",
    "    \n",
    "    net = SimpleNetwork(\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        out_size=plant.input_size,\n",
    "        noise_std=0.0,\n",
    "        out_nonlinearity=out_nonlinearity, \n",
    "        key=key1\n",
    "    )\n",
    "    \n",
    "    body = SimpleFeedback(net, mechanics, feedback_spec=feedback_spec, key=key2)\n",
    "    \n",
    "    return SimpleIterator(body, n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, if you go look in `feedbax.xabdeef.models`, you'll see that our function here is more or less the same as `point_mass_nn`. Looking inside, we can explicitly see how the model is constructed.\n",
    "\n",
    "First we construct a \"plant\", which contains all of the mathematics necessary to simulate our point mass. In this case we use a `SimplePlant`, which is a plant that has a \"skeleton\"---here, just the point mass---which is controlled directly, and not by forces generated by \"muscles\".\n",
    "\n",
    "A `Mechanics` object is used to associate the plant dynamics with a differential equation solver, with time steps discretized to the duration we specify.\n",
    "\n",
    "...     \n",
    "\n",
    "And since all of this defines just a single loop through our simulated body, we wrap it all in a `SimpleIterator`\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our function, we construct our model just like we did with `point_mass_nn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleIterator(\n",
       "  step=SimpleFeedback(\n",
       "    net=SimpleNetwork(\n",
       "      out_size=2,\n",
       "      hidden=GRUCell(\n",
       "        weight_ih=f32[150,8],\n",
       "        weight_hh=f32[150,50],\n",
       "        bias=f32[150],\n",
       "        bias_n=f32[50],\n",
       "        input_size=8,\n",
       "        hidden_size=50,\n",
       "        use_bias=True\n",
       "      ),\n",
       "      hidden_size=50,\n",
       "      noise_std=None,\n",
       "      hidden_nonlinearity=None,\n",
       "      encoder=None,\n",
       "      encoding_size=None,\n",
       "      readout=Linear(\n",
       "        weight=f32[2,50],\n",
       "        bias=f32[2],\n",
       "        in_features=50,\n",
       "        out_features=2,\n",
       "        use_bias=True\n",
       "      ),\n",
       "      out_nonlinearity=<function <lambda>>,\n",
       "      intervenors={'hidden': [], 'readout': []}\n",
       "    ),\n",
       "    mechanics=Mechanics(\n",
       "      plant=SimplePlant(\n",
       "        skeleton=PointMass(mass=1.0),\n",
       "        clip_states=True,\n",
       "        intervenors={'clip_skeleton_state': []},\n",
       "        muscle_model=None\n",
       "      ),\n",
       "      dt=0.05,\n",
       "      solver=Euler(),\n",
       "      intervenors={\n",
       "        'convert_effector_force':\n",
       "        [],\n",
       "        'statics_step':\n",
       "        [],\n",
       "        'dynamics_step':\n",
       "        [],\n",
       "        'get_effector':\n",
       "        []\n",
       "      }\n",
       "    ),\n",
       "    feedback_channels=[\n",
       "      Channel(\n",
       "        delay=1,\n",
       "        noise_std=0.1,\n",
       "        init_value=nan,\n",
       "        input_proto=(f32[2], f32[2]),\n",
       "        intervenors={'update_queue': [], 'add_noise': []}\n",
       "      )\n",
       "    ],\n",
       "    feedback_specs=[\n",
       "      ChannelSpec(where=<function <lambda>>, delay=0, noise_std=0.1)\n",
       "    ],\n",
       "    intervenors={'update_feedback': [], 'nn_step': [], 'mechanics_step': []}\n",
       "  ),\n",
       "  n_steps=100\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(\n",
    "    task,\n",
    "    dt=dt,\n",
    "    hidden_size=hidden_size,\n",
    "    n_steps=n_steps,\n",
    "    feedback_delay=feedback_delay_steps,\n",
    "    feedback_noise_std=feedback_noise_std,\n",
    "    key=key_init, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now's a good time to check out the structure of our model. Since all our models are Equinox modules, we can simply print them and the result is a nice tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleIterator(\n",
       "  step=SimpleFeedback(\n",
       "    net=SimpleNetwork(\n",
       "      out_size=2,\n",
       "      hidden=GRUCell(\n",
       "        weight_ih=f32[150,8],\n",
       "        weight_hh=f32[150,50],\n",
       "        bias=f32[150],\n",
       "        bias_n=f32[50],\n",
       "        input_size=8,\n",
       "        hidden_size=50,\n",
       "        use_bias=True\n",
       "      ),\n",
       "      hidden_size=50,\n",
       "      noise_std=None,\n",
       "      hidden_nonlinearity=None,\n",
       "      encoder=None,\n",
       "      encoding_size=None,\n",
       "      readout=Linear(\n",
       "        weight=f32[2,50],\n",
       "        bias=f32[2],\n",
       "        in_features=50,\n",
       "        out_features=2,\n",
       "        use_bias=True\n",
       "      ),\n",
       "      out_nonlinearity=<function <lambda>>,\n",
       "      intervenors={'hidden': [], 'readout': []}\n",
       "    ),\n",
       "    mechanics=Mechanics(\n",
       "      plant=SimplePlant(\n",
       "        skeleton=PointMass(mass=1.0),\n",
       "        clip_states=True,\n",
       "        intervenors={'clip_skeleton_state': []},\n",
       "        muscle_model=None\n",
       "      ),\n",
       "      dt=0.05,\n",
       "      solver=Euler(),\n",
       "      intervenors={\n",
       "        'convert_effector_force':\n",
       "        [],\n",
       "        'statics_step':\n",
       "        [],\n",
       "        'dynamics_step':\n",
       "        [],\n",
       "        'get_effector':\n",
       "        []\n",
       "      }\n",
       "    ),\n",
       "    feedback_channels=[\n",
       "      Channel(\n",
       "        delay=1,\n",
       "        noise_std=0.1,\n",
       "        init_value=nan,\n",
       "        input_proto=(f32[2], f32[2]),\n",
       "        intervenors={'update_queue': [], 'add_noise': []}\n",
       "      )\n",
       "    ],\n",
       "    feedback_specs=[\n",
       "      ChannelSpec(where=<function <lambda>>, delay=0, noise_std=0.1)\n",
       "    ],\n",
       "    intervenors={'update_feedback': [], 'nn_step': [], 'mechanics_step': []}\n",
       "  ),\n",
       "  n_steps=100\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tree reflects how we defined our model. We can see that:\n",
    "\n",
    "- The topmost level is a `SimpleIterator`, which is responsible for looping over a model step for `n_steps`.\n",
    "- The model step is an instance of `SimpleFeedback`, which is composed of a `SimpleNetwork` which sends commands to, and receives feedback from, a `Mechanics` instance\n",
    "- `SimpleNetwork` consists of a `GRUCell` containing arrays of weights, as well as a `Linear` readout with its own weights, but that `encoder=None` since by default it is constructed without a separate layer for encoding its inputs.\n",
    "- `Mechanics` is using the default `Euler` solver, and usually this is sufficient, but in some cases it might be more accurate to use one of the higher-order solvers provided by `diffrax`.\n",
    "- That each inner level of the model has a dictionary `intervenors` which contains a bunch of empty lists. Interventions will be addressed in a later example. \n",
    "\n",
    "Note that the tree does not show us *how* each part of the model performs its computations, or how it *calls* other parts of the model. It only shows which parts of the model *contain* other parts, or parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now we construct a `TaskTrainer`. We can explicitly tell the trainer to use the Adam optimizer, or any other optimizer such as provided by `optax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "from feedbax.trainer import TaskTrainer\n",
    "\n",
    "\n",
    "trainer = TaskTrainer(\n",
    "    optimizer=optax.adam(learning_rate=1e-2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our model to perform the task. \n",
    "\n",
    "We'll be explicit about which part of the model we want to be optimized: in this case, all the parameters inside of the `GRUCell` layer, and all of the parameters inside of the `Linear` readout layer. We use `lambda` to define a function, which is just a way of picking out certain parts of a `model` that is passed to that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where_train = lambda model: (\n",
    "    model.step.net.hidden,\n",
    "    model.step.net.readout,\n",
    ")\n",
    "\n",
    "model, train_history = trainer(\n",
    "    task=task, \n",
    "    model=model,\n",
    "    n_batches=2000, \n",
    "    batch_size=250, \n",
    "    log_step=200,\n",
    "    where_train=where_train,\n",
    "    key=key_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since the `GRUCell` and `Linear` layers actually contain all of the JAX arrays within our `SimpleNetwork` that could even be trained in this case, we could have equivalently written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where_train = lambda model: model.step.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and nothing would have changed. Since Feedbax is based on JAX/Equinox, we inherit a lot of flexibility in referring to subsets of models.\n",
    "\n",
    "Use of `lambda` is common in Feedbax, so it's important to recall that because a lambda is merely a convenient way of defining a function in-line, we could also have written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def where_train(model):\n",
    "    return model.step.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before passing `where_train` to `trainer`, just as we did before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example\n",
    "\n",
    "Show the code for torque control of the two-link arm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
