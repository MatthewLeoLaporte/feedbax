{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why JAX?\n",
    "\n",
    "!!! Info \"\"\n",
    "    If you are interested in Feedbax but unfamiliar with JAX—or new to Python—then keep reading, for an overview of some of the tools on which Feedbax is based.\n",
    "\n",
    "JAX isn't a machine learning framework like [PyTorch](https://pytorch.org/). It's a more general-purpose tool. \n",
    "\n",
    "??? Info \"What does JAX provide?\"\n",
    "\n",
    "    - A [NumPy](https://numpy.org/)-like API: Many of the things you can write in NumPy, you [can also write](https://jax.readthedocs.io/en/latest/jax.numpy.html) in JAX—you just have to `import jax.numpy as jnp` instead of `import numpy as np`. \n",
    "    - [Just-in-time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html): In many cases, this makes JAX much faster than NumPy. \n",
    "    - [Automatic differentiation](https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html#jax-first-transformation-grad): We use this to get derivatives of functions—usually, to train models through gradient descent.\n",
    "    - [Automatic vectorization](https://jax.readthedocs.io/en/latest/jax-101/03-vectorization.html): We can easily transform a function that works on single examples, to a function that processes entire batches of data.\n",
    "    - [Parallelism](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html): It's easy to split up a large model across multiple devices (e.g. GPUs).\n",
    "\n",
    "    Automatic differention and JIT compilation are features normally found working in the background in ML frameworks, but JAX lets you use them in explicit, arbitrary, powerful ways.\n",
    "\n",
    "\n",
    "That's why Feedbax is not just built on JAX, but also:\n",
    "\n",
    "- [Equinox](https://github.com/patrick-kidger/equinox), which allows us to define PyTorch-like modules, making it easiers to organize our models;\n",
    "- [Optax](https://github.com/google-deepmind/optax), which provides optimizers (like Adam) which you'd normally find in ML frameworks;\n",
    "- [Diffrax](https://github.com/patrick-kidger/diffrax), which provides numerical solvers for differential equations.\n",
    "\n",
    "My favourite part about working with JAX is how nicely it plays with nested containers of data, or [*PyTrees*](https://jax.readthedocs.io/en/latest/pytrees.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytrees\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a list and a dict that contain similar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_list = [1, 2, 3]\n",
    "\n",
    "some_dict = {'a': 1, 'b': 2, 'c': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In standard Python, a [*comprehension*](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions) is a typical way of applying some computation to every value in a list or dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x ** 2 for x in some_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 4, 'c': 9}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: x ** 2 for k, x in some_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While list and dict comprehensions are similar, they're not interchangeable. If our data is a list we can use the first method and get a list in return. But as soon as we introduce some data that's stored in a dict, we need to change our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Conveniently, JAX provides a function [`tree_map`](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.tree_map.html) that behaves the same way for both lists and dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax.tree_util import tree_map\n",
    "\n",
    "tree_map(lambda x: x ** 2, some_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 4, 'c': 9}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_map(lambda x: x ** 2, some_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "??? Note \"Python's built-in `map`\"\n",
    "    Python includes a built-in function [`map`](https://docs.python.org/3/library/functions.html#map) which is similar in principle to `tree_map`. For example, we can do `list(map(lambda x: x**2, some_list))` to get the same result as `tree_map(lambda x: x**2, some_list)`. \n",
    "    \n",
    "    However:\n",
    "    \n",
    "    - `map` doesn't necessarily return the same type of data structure passed to it. To square all the values in a dict, and return a dict like we did with `tree_map`, we'd have to do something like `dict(zip(some_dict.keys(), map(lambda x: x**2, some_dict.values())))`. That's much less readable than the JAX solution.\n",
    "    - `map` only works one level deep into a container. This isn't an issue with `some_list`, which is a flat list of numbers. But in many other cases the data we'll want to transform will be in complex, nested trees.\n",
    "\n",
    "Even better, `tree_map` works on nested containers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'p': [1, 4], 'x': 1.0},\n",
       " [25, 36, 49, 64, {'y': Array([4, 4, 4], dtype=int32)}]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "some_data = [{'p': [1, 2], 'x': 1.0}, [5, 6, 7, 8, {'y': jnp.array([2, 2, 2])}]]\n",
    "\n",
    "tree_map(lambda x: x ** 2, some_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this work? JAX treats both lists and dicts—*and any nested structures of lists and dicts*—as PyTrees. \n",
    "\n",
    "A PyTree's *leaves* are the data it ultimately contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1.0, 5, 6, 7, 8, Array([2, 2, 2], dtype=int32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax.tree_util import tree_leaves, tree_structure\n",
    "\n",
    "tree_leaves(some_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those leaves are arranged in a tree with a certain structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTreeDef([{'p': [*, *], 'x': *}, [*, *, *, *, {'y': *}]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_structure(some_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the JAX array counts as a leaf, not as part of the tree structure. What does JAX treat as a leaf, and what does it treat as the structure? \n",
    "\n",
    "By default:\n",
    "\n",
    "- *leaves*—AKA *leaf nodes*—include NumPy and JAX arrays, as well as basic data types like `int`, `float`, `str`, and `bool`;\n",
    "- *internal nodes* are lists, dicts, and tuples, which JAX recognizes as PyTrees themselves. When JAX encounters these, its default stance is that \"nesting continues here\"—so it looks inside the node for leaves, or even deeper layers of nodes.\n",
    "\n",
    "Importantly, we can change what counts as a leaf. Many functions that operate on PyTrees can take an argument `is_leaf`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'p': [1, 2], 'x': 1.0}, 5, 6, 7, 8, {'y': Array([2, 2, 2], dtype=int32)}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_leaves(some_data, is_leaf=lambda x: isinstance(x, dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTreeDef([*, [*, *, *, *, *]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_structure(some_data, is_leaf=lambda x: isinstance(x, dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've told JAX to treat dicts as leaves, rather than as containers. Now, the dicts appear whole and unflattened in the list of leaves, and the PyTree structure reflects this. \n",
    "\n",
    "To get the leaves and the tree structure in one call, use [`tree_flatten`](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.tree_flatten.html#jax.tree_util.tree_flatten):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import tree_flatten\n",
    "\n",
    "leaves, structure = tree_flatten(some_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given both leaves and the structure, [`tree_unflatten`](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.tree_unflatten.html#jax.tree_util.tree_unflatten) builds a PyTree. Let's reconstruct the original `some_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'p': [1, 2], 'x': 1.0}, [5, 6, 7, 8, {'y': Array([2, 2, 2], dtype=int32)}]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax.tree_util import tree_unflatten\n",
    "\n",
    "tree_unflatten(structure, leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because JAX understands the structure of PyTrees, we can apply operations to multiple PyTrees when their structures match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_arrays = [\n",
    "    (jnp.array([1, 2]), jnp.array([3, 4])),\n",
    "    jnp.array([5, 6])\n",
    "]\n",
    "\n",
    "some_other_arrays = [\n",
    "    (jnp.array([7, 8]), jnp.array([3, 4])),\n",
    "    jnp.array([1, 1])\n",
    "]\n",
    "\n",
    "tree_structure(some_arrays) == tree_structure(some_other_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Array([ 8, 10], dtype=int32), Array([6, 8], dtype=int32)),\n",
       " Array([6, 7], dtype=int32)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_map(\n",
    "    lambda x, y: x + y,\n",
    "    some_arrays,\n",
    "    some_other_arrays\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `tree_map` works \"leafwise\" to pick out the arguments to a function: the `x` values are the leaves from `some_arrays`, and the `y` values are the matching leaves from `some_other_arrays`. \n",
    "\n",
    "In this example, the result will be different if we tell JAX to treat tuples as leaves. The first two JAX arrays in each PyTree are contained in a tuple, so the first `x` passed to the function will be a pair of tuples, as will the first `y`. When we apply `+` to two tuples, we concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Array([1, 2], dtype=int32),\n",
       "  Array([3, 4], dtype=int32),\n",
       "  Array([7, 8], dtype=int32),\n",
       "  Array([3, 4], dtype=int32)),\n",
       " Array([6, 7], dtype=int32)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_map(\n",
    "    lambda x, y: x + y,\n",
    "    some_arrays,\n",
    "    some_other_arrays,\n",
    "    is_leaf=lambda x: isinstance(x, tuple)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array that's not inside a tuple gets added the same way it did before, because it still counts as a leaf—it's just that now, tuples *also* count as leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A PyTree of your own\n",
    "\n",
    "A PyTree is any kind of container that JAX knows how to flatten and unflatten. By default, this includes lists, dicts, and tuples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the default containers aren't enough for us, we can define our own types of containers, and tell JAX how to flatten and unflatten them. After that, JAX will treat them as PyTrees!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import register_pytree_node_class\n",
    "\n",
    "@register_pytree_node_class\n",
    "class TwoValues:\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def tree_flatten(self):\n",
    "        return (self.a, self.b), None  # leaves, aux_data\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, leaves):\n",
    "        print(aux_data)\n",
    "        return cls(*leaves)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the method `tree_flatten` tells JAX how to flatten a `TwoValues` object into its leaves, and `tree_unflatten` tells how to construct `TwoValues` given the leaves.\n",
    "\n",
    "As its name suggests, the decorator [`register_pytree_node_class`](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.register_pytree_node_class.html#jax.tree_util.register_pytree_node_class) registers our new PyTree type with JAX.\n",
    "\n",
    "Now we can use `TwoValues` as part of any PyTree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 45, 3, 4, 5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = (TwoValues(12, 45), 3, {'a': TwoValues(4, 5)})\n",
    "\n",
    "tree_leaves(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTreeDef((CustomNode(TwoValues[None], [*, *]), *, {'a': CustomNode(TwoValues[None], [*, *])}))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_structure(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equinox\n",
    "\n",
    "!!! Note inline end \"\"\n",
    "    Most objects in Feedbax are derived from `equinox.Module`.\n",
    "\n",
    "[Equinox](https://docs.kidger.site/equinox/) adds some useful tools to JAX. \n",
    "\n",
    "In particular, [`equinox.Module`](https://docs.kidger.site/equinox/api/module/module/) allows us to easily define classes that are PyTrees, and that combine model parameters with model computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "\n",
    "\n",
    "class SomeModel(eqx.Module):\n",
    "    param1: int\n",
    "    param2: jax.Array\n",
    "\n",
    "    def __call__(self, x: float):\n",
    "        return self.param1 + x * self.param2\n",
    "\n",
    "\n",
    "# Construct an example model.\n",
    "model = SomeModel(3, jnp.array([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our class definition, the method `__call__` tells Python how a `SomeModel` object should behave, when we call it like a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 5.5,  8. , 10.5], dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice way to define and execute our model computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another convenient thing about Equinox `Module` is that it's a [`dataclass`](https://docs.python.org/3/library/dataclasses.html). In a normal Python class, to assign `param1` and `param2` as instance attributes we'd have to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeModel:\n",
    "    def __init__(self, param1: int, param2: jax.Array):\n",
    "        self.param1 = param1\n",
    "        self.param2 = param2\n",
    "\n",
    "    def __call__(self, x: float):\n",
    "        return self.param1 + x * self.param2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our class is a dataclass, it automatically defines a default `__init__` method like the one above. We just have to define the list of parameters (that is, dataclass *fields*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SomeModel:\n",
    "    param1: int\n",
    "    param2: jax.Array\n",
    "\n",
    "    def __call__(self, x: float):\n",
    "        return self.param1 + x * self.param2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any class or subclass we define from `eqx.Module` will automatically work this way, without needing to add the `@dataclass` decorator. \n",
    "\n",
    "!!! Note \n",
    "    We can still add our own `__init__` method to a dataclass if we need to do something fancier than just assigning values to fields. \n",
    "    \n",
    "    In case only small modifications to `__init__` are needed, it may be convenient to define [`__post_init__`](https://docs.python.org/3/library/dataclasses.html#dataclasses.__post_init__) instead.\n",
    "\n",
    "The best thing about Equinox modules is that they are PyTrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, Array([1, 2, 3], dtype=int32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a flattened list of model parameters.\n",
    "tree_leaves(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out this is very useful for structuring models, but that's beyond the scope of this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "??? Note \"Similarity of Equinox and PyTorch modules\"\n",
    "    Equinox's [`Module`](https://docs.kidger.site/equinox/api/module/module/#equinox.Module) is kind of like PyTorch's [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). However, PyTorch modules:\n",
    "    \n",
    "    - are not PyTrees, because PyTorch has no general, built-in concept of PyTrees;\n",
    "    - are not automatically dataclasses, and it can be kind of [problematic](https://discuss.pytorch.org/t/how-to-use-dataclass-with-pytorch/53444/9) to convert them;\n",
    "    - define the model computation in the `forward` method, rather than `__call__`. Technically though, PyTorch still has to define `__call__` in the background to have its module objects behave like functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation and `vmap`\n",
    "\n",
    "The power of pytrees goes much deeper than we've seen here. The core JAX transformations, jax.vmap and jax.grad\n",
    "\n",
    "!!! NOTE    \n",
    "    If you run into problems with `jax.vmap`, try using Equinox's `filter_vmap` as we've done above. It does the same thing, but a little more intelligently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and states\n",
    "\n",
    "JAX [plays best](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions) with [pure functions](https://en.wikipedia.org/wiki/Purely_functional_programming). Let's see what that means.\n",
    "\n",
    "Perhaps you are familiar with object-oriented programming, where *classes* define how objects possess and manipulate their *internal states*. For example, let's define a type of object that 1) possesses two attributes, and 2) when it's called, returns a result, but also internally updates one of its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tx\tsmee\n",
      "Step 0:\t\t2\t0\n",
      "Step 1:\t\t4\t0\n",
      "Step 2:\t\t8\t2\n",
      "Step 3:\t\t16\t2\n",
      "Step 4:\t\t32\t2\n",
      "Step 5:\t\t64\t2\n",
      "Step 6:\t\t128\t2\n"
     ]
    }
   ],
   "source": [
    "class StatefulFoo:\n",
    "    smee: int\n",
    "    a: int\n",
    "\n",
    "    def __init__(self, a: int):\n",
    "        self.smee = 0\n",
    "        self.a = a\n",
    "\n",
    "    def __call__(self, x: int):\n",
    "\n",
    "        if x > 3:\n",
    "            self.smee = 2\n",
    "\n",
    "        return self.a * x\n",
    "\n",
    "\n",
    "a = 2\n",
    "foo = StatefulFoo(a)\n",
    "x = 1\n",
    "\n",
    "print(\"\\t\\tx\\tsmee\")\n",
    "\n",
    "for i in range(7):\n",
    "    x = foo(x)\n",
    "\n",
    "    print(f\"Step {i}:\\t\\t{x}\\t{foo.smee}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"smee\"></a>Importantly, the internal state—the value of `foo.smee`—changes once a certain value is passed to `foo`. This is obvious in this case, since we're printing `foo.smee` on every step. But under different circumstances, we might not even know it had changed. \n",
    "\n",
    "Seen as a function, the main thing that `foo` does is to return `result`. But it also has the *side effect* of altering `foo.smee`.\n",
    "\n",
    "On the other hand, *a pure function does not have side effects*. Everything that the function does, is how its input gets turned into its return value. \n",
    "\n",
    "We can still do what we did with `foo.smee`, except that `smee` can no longer be hidden. It just needs to be part of the input and output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tx\tsmee\n",
      "Step 0:\t\t2\t0\n",
      "Step 1:\t\t4\t0\n",
      "Step 2:\t\t8\t2\n",
      "Step 3:\t\t16\t2\n",
      "Step 4:\t\t32\t2\n",
      "Step 5:\t\t64\t2\n",
      "Step 6:\t\t128\t2\n"
     ]
    }
   ],
   "source": [
    "class PureFoo:\n",
    "    a: int\n",
    "\n",
    "    def __init__(self, a: int):\n",
    "        self.a = a\n",
    "\n",
    "    def __call__(self, x: int, smee: int):\n",
    "\n",
    "        if x > 3:\n",
    "            smee = 2\n",
    "\n",
    "        return self.a * x, smee\n",
    "\n",
    "a = 2\n",
    "foo = PureFoo(a)\n",
    "smee = 0\n",
    "x = 1\n",
    "\n",
    "print(\"\\t\\tx\\tsmee\")\n",
    "\n",
    "for i in range(7):\n",
    "    x, smee = foo(x, smee)\n",
    "\n",
    "    print(f\"Step {i}:\\t\\t{x}\\t{smee}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe this doesn't seem as nice as `StatefulFoo`, but it is totally transparent. And if we keep building up our programs in this way, it forces us to start adding more structure to the inputs and outputs of our functions. <a name=\"purefoo\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tx\tsmee\n",
      "Step 0:\t\t2\t0\n",
      "Step 1:\t\t4\t0\n",
      "Step 2:\t\t8\t2\n",
      "Step 3:\t\t16\t2\n",
      "Step 4:\t\t32\t2\n",
      "Step 5:\t\t64\t2\n",
      "Step 6:\t\t128\t2\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Data:\n",
    "    x: int\n",
    "    smee: int\n",
    "\n",
    "\n",
    "class PureFoo:\n",
    "    a: int\n",
    "\n",
    "    def __init__(self, a: int):\n",
    "        self.a = a\n",
    "\n",
    "    # Takes Data, and returns Data.\n",
    "    def __call__(self, data: Data) -> Data:\n",
    "\n",
    "        if data.x > 3:\n",
    "            smee = 2\n",
    "        else:\n",
    "            smee = data.smee\n",
    "\n",
    "        return Data(2 * data.x, smee)\n",
    "\n",
    "\n",
    "a = 2\n",
    "foo = PureFoo(a)\n",
    "data = Data(x=1, smee=0)\n",
    "\n",
    "print(\"\\t\\tx\\tsmee\")\n",
    "\n",
    "for i in range(7):\n",
    "    data = foo(data)\n",
    "\n",
    "    print(f\"Step {i}:\\t\\t{data.x}\\t{data.smee}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that as our programs grow complex, this style will work at least as well as the stateful style ever did—and without hiding anything.\n",
    "\n",
    "!!! Note \"\"\n",
    "    In Feedbax, the relationship between a model and its state is like the relationship between `PureFoo` and `Data`, in this example. A model does not *possess* state, it *operates* on it. \n",
    "    \n",
    "    Similarly, we never change a state object by directly reassigning its values. For example, in the above example we would never do this:\n",
    "    \n",
    "    ```python\n",
    "    data = Data(x=1, smee=0)\n",
    "    data.smee = 2\n",
    "    ```\n",
    "    \n",
    "    As we'll see shortly, this won't be a problem. We'll just need to define the alteration to `data` as some function that takes `data` as its input, and constructs the altered version as its output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equinox and pure functions\n",
    "\n",
    "It might seem a little odd that we contrasted object oriented programming with purely functional programming, and then we kept defining our \"pure function\" as a  `class`!\n",
    "\n",
    "It's not really odd, though. What matters is that our classes *behave* like pure functions because of the way we define `__call__`. And classes do one very convenient thing for us: they let us keep fixed model parameters (like `a`) in the same place as a method that defines the model's computation.\n",
    "\n",
    "This is essentially what `eqx.Module` is for. And it forces us to code in a functional style. Watch what happens if we try to change one of the attributes of an Equinox module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FrozenInstanceError",
     "evalue": "cannot assign to field 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFrozenInstanceError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m     a: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m      4\u001b[0m my_bar \u001b[38;5;241m=\u001b[39m Bar(a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmy_bar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n",
      "\u001b[0;31mFrozenInstanceError\u001b[0m: cannot assign to field 'a'"
     ]
    }
   ],
   "source": [
    "class Bar(eqx.Module):\n",
    "    a: int\n",
    "\n",
    "my_bar = Bar(a=3)\n",
    "\n",
    "my_bar.a = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things are no different if the object tries to change itself directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FrozenInstanceError",
     "evalue": "cannot assign to field 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFrozenInstanceError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m      8\u001b[0m my_baz \u001b[38;5;241m=\u001b[39m Baz(a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmy_baz\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[93], line 5\u001b[0m, in \u001b[0;36mBaz.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma\u001b[49m \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n",
      "\u001b[0;31mFrozenInstanceError\u001b[0m: cannot assign to field 'a'"
     ]
    }
   ],
   "source": [
    "class Baz(eqx.Module):\n",
    "    a: int\n",
    "\n",
    "    def __call__(self, x: int):\n",
    "        self.a = x\n",
    "\n",
    "\n",
    "my_baz = Baz(a=3)\n",
    "\n",
    "my_baz(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, Equinox modules are *immutable*. Immutability goes hand in hand with pure functions, because it ensures that the internal state of our objects cannot be altered in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model is data, too\n",
    "\n",
    "We've been referring to `a` as a fixed parameter. [Earlier](#purefoo) in this example, `PureFoo` was allowed to change `data`, but not its own `a`!\n",
    "\n",
    "We can still change `a`. We just need another kind of function, that operates on `PureFoo`, like `PureFoo` operated on `Data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old a: 2\n",
      "New a: 3\n"
     ]
    }
   ],
   "source": [
    "def foo_update(foo: PureFoo) -> PureFoo:\n",
    "    a = foo.a + 1\n",
    "    return PureFoo(a)\n",
    "\n",
    "\n",
    "foo = PureFoo(a=2)\n",
    "foo_new = foo_update(foo)\n",
    "\n",
    "\n",
    "print(f\"Old a: {foo.a}\")\n",
    "print(f\"New a: {foo_new.a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we've just defined a plain old Python function with `def`, but we could also have using an Equinox module to define this function, if it had parameters of its own to remember."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAX arrays are immutable\n",
    "\n",
    "Here's something we can do in NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 5., 5.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "some_array = np.zeros((3, 3))\n",
    "\n",
    "# Modify the array in-place.\n",
    "some_array[0, 1:] = 5\n",
    "\n",
    "some_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same thing doesn't work in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m some_array \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m \u001b[43msome_array\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n",
      "File \u001b[0;32m~/.miniforge3/envs/fx/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:285\u001b[0m, in \u001b[0;36m_unimplemented_setitem\u001b[0;34m(self, i, x)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unimplemented_setitem\u001b[39m(\u001b[38;5;28mself\u001b[39m, i, x):\n\u001b[1;32m    281\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object does not support item assignment. JAX arrays are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimmutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor another .at[] method: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 285\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)))\n",
      "\u001b[0;31mTypeError\u001b[0m: '<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "some_array = jnp.zeros((3, 3))\n",
    "\n",
    "some_array[0, 1:] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX arrays are immutable! That means we can't reach in and change an array *in-place*. We always have to perform some transformation that returns a new array.\n",
    "\n",
    "JAX provides the `at-set` syntax for assigning a value to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 5., 5.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_array = some_array.at[0, 1:].set(5)\n",
    "\n",
    "some_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right hand side of the assignment can be seen as a function that takes `some_array`, and returns a new array object with the requested alteration.\n",
    "\n",
    "In some cases it might appear that JAX performs in-place operations. For example, in NumPy we can do in-place addition like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3.],\n",
       "       [3., 3.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_array = np.zeros((2, 2))\n",
    "\n",
    "another_array += 3\n",
    "\n",
    "another_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell that the operation is in-place because `another_array` has the same object ID as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_before = id(another_array)\n",
    "\n",
    "another_array += 10\n",
    "\n",
    "id_after = id(another_array)\n",
    "\n",
    "id_before == id_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, this is not the case in JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now using jnp, not np!\n",
    "another_array = jnp.zeros((2, 2))\n",
    "\n",
    "id_before = id(another_array)\n",
    "\n",
    "another_array += 10\n",
    "\n",
    "id_after = id(another_array)\n",
    "\n",
    "id_before == id_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different ID means a different Python object. In other words, JAX treats `another_array += 10` like it would treat the purely functional `another_array = another_array + 10`, and not as an in-place update. \n",
    "\n",
    "To be clear, you should write `another_array = another_array + 10`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immutability in Feedbax: performing surgery\n",
    "\n",
    "In Feedbax, models and states can be really big PyTrees. Often we want to change just one part of them. But we don't want to keep writing huge functions that reconstruct the entire model, every time we want to replace just one piece.\n",
    "\n",
    "Thankfully, Equinox provides a general-purpose function that can *perform surgery*.\n",
    "\n",
    "Let's start with a [pre-built model](/feedbax/examples/0_train_simple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Unable to load CUDA. Is it installed? (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/home/mll/.miniforge3/envs/fx/lib/python3.11/site-packages/diffrax/adjoint.py:665: UserWarning: As of Equinox 0.10.7, `equinox.filter_custom_vjp.defvjp` is deprecated in favour of `.def_fwd` and `.def_bwd`. This new API supports symbolic zeros, which allow for more efficient autodifferentiation rules. In particular:\n",
      "- the fwd and bwd functions take an extra `perturbed` argument, which     indicates which primals actually need a gradient. You can use this     to skip computing the gradient for any unperturbed value. (You can     also safely just ignore this if you wish.)\n",
      "- `None` was previously passed to indicate a symbolic zero gradient for     all objects that weren't inexact arrays, but all inexact arrays     always had an array-valued gradient. Now, `None` may also be passed     to indicate that an inexact array has a symbolic zero gradient.\n",
      "  _loop_backsolve.defvjp(_loop_backsolve_fwd, _loop_backsolve_bwd)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "from feedbax.xabdeef import point_mass_nn_simple_reaches\n",
    "\n",
    "\n",
    "context = point_mass_nn_simple_reaches(key=jax.random.PRNGKey(0))\n",
    "model = context.model  # Shorthand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a point mass of mass $1.0$ as its skeleton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointMass(mass=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.step.mechanics.plant.skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, if we try to directly alter the model to use a point mass of mass $5.0$, an error is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FrozenInstanceError",
     "evalue": "cannot assign to field 'skeleton'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mFrozenInstanceError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeedbax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmechanics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mskeleton\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PointMass \n",
      "\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Try to replace the entire point mass\u001b[39;00m\n",
      "\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmechanics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskeleton\u001b[49m \u001b[38;5;241m=\u001b[39m PointMass(\u001b[38;5;241m5.0\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n",
      "\n",
      "\u001b[0;31mFrozenInstanceError\u001b[0m: cannot assign to field 'skeleton'"
     ]
    }
   ],
   "source": [
    "from feedbax.mechanics.skeleton import PointMass\n",
    "\n",
    "# Try to replace the entire point mass\n",
    "model.step.mechanics.plant.skeleton = PointMass(5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FrozenInstanceError",
     "evalue": "cannot assign to field 'mass'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mFrozenInstanceError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Try to just change the mass\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmechanics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskeleton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmass\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5.0\u001b[39m\n",
      "\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n",
      "\n",
      "\u001b[0;31mFrozenInstanceError\u001b[0m: cannot assign to field 'mass'"
     ]
    }
   ],
   "source": [
    "# Or just try to change the mass\n",
    "model.step.mechanics.plant.skeleton.mass = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we use the [`tree_at`](https://docs.kidger.site/equinox/api/manipulation/#equinox.tree_at) function from Equinox. This is a function that takes a PyTree, and replaces a piece of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "\n",
    "model_heavy = eqx.tree_at(\n",
    "    lambda m: m.step.mechanics.plant.skeleton,\n",
    "    model,\n",
    "    PointMass(5.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument to `tree_at` is a \"locator\" function: when we pass it the model, it returns the part of the model we want to replace. \n",
    "\n",
    "!!! Note \"Lambda functions\"\n",
    "    Using Python's `lambda` syntax lets us define the function inline. This isn't strictly necessary, but it's common practice in JAX when we need to define functions to pick out parts of PyTrees.\n",
    "\n",
    "The second argument is just `model`, which is the model we want to alter. \n",
    "\n",
    "The third argument is the replacement part.\n",
    "\n",
    "Here, the model with the modifications is assigned to `model_heavy`, and we can still refer to the original model as `model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random number generation\n",
    "\n",
    "One other way that JAX differs from NumPy is how it handles the generation of random numbers. \n",
    "\n",
    "In NumPy, random number generators are stateful. We can see this by calling one more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6916782346283932"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26970965558017235"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two numbers are different, but the input to the function was not: in both cases, we passed no arguments. \n",
    "\n",
    "If a function's output differs when the input remains the same, it's not a pure function. In this case, the number changed because it was based on a state variable that changed in the background, like `smee` did [earlier](#smee). \n",
    "\n",
    "These aren't actually random numbers, they're [pseudo-random](https://en.wikipedia.org/wiki/Pseudorandomness): they're the outputs of a deterministic function that varies wildly with its input. In NumPy, we can control where it starts from by setting the random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1915194503788923"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1234\n",
    "\n",
    "np.random.seed(seed)\n",
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6221087710398319"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1915194503788923"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6221087710398319"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we set the seed, the random numbers start from the same point. This makes our subsequent calls reproducible, in principle. However, because the state of the random number generator continues to change in the background, our code may stop being reproducible if at any point during the execution of our program, some other program makes even a single call to the random number generator, and changes its state.\n",
    "\n",
    "!!! Note  \n",
    "    The situation is similar in PyTorch, and the above example can be repeated with `torch.rand` and `torch.manual_seed`.\n",
    "    \n",
    "JAX takes a totally functional and transparent approach to random numbers. Whenever we want to generate a random number, we have to pass a *key*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.376078, dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.random as jr\n",
    "\n",
    "seed = 5678\n",
    "key = jr.PRNGKey(seed)\n",
    "\n",
    "jr.uniform(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call a random generator a second time with the same key, it returns the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.376078, dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jr.uniform(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to generate a new random number, we get a new key with `split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6928469\n",
      "0.040529132\n"
     ]
    }
   ],
   "source": [
    "key1, key2 = jr.split(key)\n",
    "\n",
    "print(jr.uniform(key1))\n",
    "print(jr.uniform(key2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This forces us to always be clear about the logic of how random numbers are generated. In JAX it's typical to see something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(key):\n",
    "    key_uniform, key_normal = jr.split(key)\n",
    "    data_uniform = jr.uniform(key_uniform, (2, 4))\n",
    "    data_normal = jr.normal(key_normal, (2, 4))\n",
    "    return {'uniform': data_uniform, 'normal': data_normal}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally we only pass a single key to a function, if the function needs to generate random numbers internally. The function defines how many keys it actually needs, by splitting the one we send it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uniform': Array([[0.1093446 , 0.97192943, 0.60279703, 0.5552217 ],\n",
       "        [0.4859867 , 0.6875596 , 0.18040001, 0.6805732 ]], dtype=float32),\n",
       " 'normal': Array([[-0.92087466, -0.99356407, -0.01340629,  0.2917211 ],\n",
       "        [-0.09446456, -0.53876567,  0.04995674, -0.8308685 ]],      dtype=float32)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = jr.PRNGKey(seed)\n",
    "\n",
    "generate_data(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uniform': Array([[0.1093446 , 0.97192943, 0.60279703, 0.5552217 ],\n",
       "        [0.4859867 , 0.6875596 , 0.18040001, 0.6805732 ]], dtype=float32),\n",
       " 'normal': Array([[-0.92087466, -0.99356407, -0.01340629,  0.2917211 ],\n",
       "        [-0.09446456, -0.53876567,  0.04995674, -0.8308685 ]],      dtype=float32)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the same key for reproducible results.\n",
    "generate_data(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to random number generation in NumPy and PyTorch, I find that JAX takes a small amount of extra effort—in most cases, writing at most 1 extra line of code per function, to split keys—but I end up feeling significantly more comfortable that my code's output is actually reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
